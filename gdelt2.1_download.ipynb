{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0af382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Software\\anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "D:\\Software\\anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "D:\\Software\\anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from zipfile import BadZipFile\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import io\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import psycopg2\n",
    "import logging\n",
    "import requests\n",
    "import psutil\n",
    "import logging\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import csv\n",
    "from io import StringIO\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "#curdir = os.path.abspath(os.getcwd())\n",
    "#if not curdir.endswith(os.sep):\n",
    "#    curdir += os.sep\n",
    "# http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\n",
    "\n",
    "curdir=\"/Volumes/T5EVO/gdelt_download/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8718585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def process_url(url):\n",
    "    try:\n",
    "        # Split to get the filename\n",
    "        name = url.split(\"/\")[-1]\n",
    "        names = name.split(\".\")\n",
    "\n",
    "        # Download the file\n",
    "        download_url(url, curdir + name)\n",
    "\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(curdir + name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(curdir + names[0])\n",
    "\n",
    "        # Read the CSV file and process data\n",
    "        news = pd.read_csv(curdir + \n",
    "                           names[0] + '/' + \n",
    "                           names[0] + \".\" + \n",
    "                           names[1] + \".\" + \n",
    "                           names[2], \n",
    "                           on_bad_lines='skip', \n",
    "                           delimiter=\"\\t\", names=[\"GKGRECORDID\", \"DATE\", \n",
    "                                                  \"SourceCollectionIdentifier\", \"SourceCommonName\", \n",
    "                                                  \"DocumentIdentifier\", \"Counts\", \"V2Counts\", \n",
    "                                                  \"Themes\", \"V2Themes\", \"Locations\", \"V2Locations\", \n",
    "                                                  \"Persons\", \"V2Persons\", \"Organizations\", \"V2Organizations\", \n",
    "                                                  \"V2Tone\", \"Dates\", \"GCAM\", \"SharingImage\", \"RelatedImages\", \n",
    "                                                  \"SocialImageEmbeds\", \"SocialVideoEmbeds\", \"Quotations\", \n",
    "                                                  \"AllNames\", \"Amounts\", \"TranslationInfo\", \"Extras\"], \n",
    "                                                  dtype=\"string\", \n",
    "                                                  encoding = \"ISO-8859-1\")\n",
    "        # \"GKGRECORDID\", \"DATE\", \n",
    "        # \"SourceCollectionIdentifier\", \"SourceCommonName\", \n",
    "        # \"DocumentIdentifier\", \"Counts\", \"V2Counts\", \n",
    "        # \"Themes\", \"V2Themes\", \"Locations\", \"V2Locations\", \n",
    "        # \"Persons\", \"V2Persons\", \"Organizations\", \"V2Organizations\", \n",
    "        # \"V2Tone\", \"Dates\", \"GCAM\", \"SharingImage\", \"RelatedImages\", \n",
    "        # \"SocialImageEmbeds\", \"SocialVideoEmbeds\", \"Quotations\", \n",
    "        # \"AllNames\", \"Amounts\", \"TranslationInfo\", \"Extras\"\n",
    "\n",
    "        # Clean up the downloaded and extracted files\n",
    "        #shutil.move(curdir + name, f'{curdir}zip_2024/{name}') # save the zip to target file instead of deleting\n",
    "        os.remove(curdir + name)\n",
    "        shutil.rmtree(curdir + names[0])\n",
    "\n",
    "        return news, name, names\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing {url}: {e}\")\n",
    "        # Write the URL to a text file\n",
    "        with open('error_urls.txt', 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "        return False  # Indicate failure\n",
    "    \n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "def generate_gdelt_urls(start_year, start_month, start_day, end_year, end_month, end_day):\n",
    "    start_date = datetime(start_year, start_month, start_day)\n",
    "    current_date = datetime(end_year, end_month, end_day) # change to the end of the year\n",
    "\n",
    "    delta = current_date - start_date\n",
    "    total_intervals = int(delta.total_seconds() / 900)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for i in range(total_intervals):\n",
    "        interval_date = start_date + timedelta(minutes = 15 * i)\n",
    "\n",
    "        formatted_date = interval_date.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "        url = f'http://data.gdeltproject.org/gdeltv2/{formatted_date}.gkg.csv.zip'\n",
    "        urls.append(url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def clean_bot(x):\n",
    "    \"\"\"\n",
    "    The actual robot which is gonna clean the company suffix for every company.\n",
    "    :type x: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if len(x) > 0 and x[-1] in redundant:\n",
    "        del x[-1]\n",
    "        clean_bot(x)\n",
    "    return x\n",
    "\n",
    "def clean(x):\n",
    "    \"\"\"\n",
    "    Clean all the organizations from the 'Organizations' column in the raw data file.\n",
    "    The data type of every entry in the 'Organizations' column is string.\n",
    "    :type x: str\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    x = x.split(';')\n",
    "    temp = list()\n",
    "    for y in x:\n",
    "        y = y.split(' ')\n",
    "        temp.append(clean_bot(y))\n",
    "    return temp\n",
    "\n",
    "def del_countries(x):\n",
    "    \"\"\"\n",
    "    Delete the strings which are names of countries in every entry from the 'Organizations' column.\n",
    "    :type: list (list of lists)\n",
    "    :rtype: list (list of lists)\n",
    "    \"\"\"\n",
    "    x = [k for k in x if ' '.join(k).lower() not in nations]\n",
    "    return x\n",
    "\n",
    "def find_index(corp_name, middle_index, company_key1):\n",
    "    \"\"\"\n",
    "    Precisely find the index of the objective string (company name) by setting ',' as delimiter and loop the string.\n",
    "    Return the index if found, return -1 if not found.\n",
    "    :type: string, int, string\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    middle2 = company_key1[middle_index:].find(corp_name)\n",
    "    if middle2 == -1:\n",
    "        return middle2\n",
    "    elif middle2 == 0:\n",
    "        return middle2\n",
    "    elif company_key1[middle2 + middle_index - 1] == ',':\n",
    "        return middle2 + middle_index\n",
    "    else:\n",
    "        index1 = company_key1[middle2 + middle_index:].find(',')\n",
    "        if index1 == -1:\n",
    "            return -1\n",
    "        count1 = middle2 + middle_index + index1\n",
    "        return find_index(corp_name, count1, company_key1)\n",
    "\n",
    "def company_matching(x, company_key1, company_dict):\n",
    "    \"\"\"\n",
    "    Match the objective string (company name) and return a list of found companies.\n",
    "    :type: list (list of lists)\n",
    "    :type company_key1: string\n",
    "    :type company_dict: dictionary\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    x1 = [' '.join(y) for y in x]\n",
    "    childs = list()\n",
    "    for i in x1:\n",
    "        head_index = find_index(i, 0, company_key1)\n",
    "\n",
    "        if head_index == -1:\n",
    "            continue\n",
    "        else:\n",
    "            if company_key1[head_index:].find(',') == -1:\n",
    "                childs.append(company_key1[head_index:])\n",
    "            else:\n",
    "                childs.append(company_key1[head_index:head_index + company_key1[head_index:].find(',')])\n",
    "\n",
    "    if len(childs) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        return childs\n",
    "\n",
    "def return_first_v2tone_score(x):\n",
    "    \"\"\"\n",
    "    Return the first numerical value of a string in an entry of V2Tone column from raw data file.\n",
    "    :type: string\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # l = x.split(',')\n",
    "    return float(x.split(',')[0])\n",
    "\n",
    "def clean_raw_data(data):\n",
    "    news = data#[['DATE', 'DocumentIdentifier', 'V2Tone', 'Themes', 'Organizations', \"V2Locations\"]]\n",
    "    # Drop missing value for org and themes; drop duplicate value for V2Tone(there are duplicate news which can affect our research performance)\n",
    "    news_clean = news[~news['Organizations'].isna()]\n",
    "    news_clean = news_clean[~news_clean['Themes'].isna()]\n",
    "\n",
    "    # print('Cleaning corp...')\n",
    "    index = news_clean['Organizations'].apply(lambda x: clean(x))\n",
    "    news_clean['Organizations'] = index\n",
    "\n",
    "    # print('Cleaning countries...')\n",
    "    index = news_clean['Organizations'].apply(del_countries)\n",
    "    news_clean['Organizations'] = index\n",
    "\n",
    "    # print('Matching companies...')\n",
    "    index = news_clean['Organizations'].apply(lambda x: company_matching(x, company_key1, company_dict))\n",
    "    news_clean['Organizations'] = index\n",
    "    # Drop missing values\n",
    "    news_clean = news_clean.loc[index[~index.isnull()].index]\n",
    "    # Drop duplicates based on tone,themes,organization\n",
    "    news_clean['Organizations'] = news_clean['Organizations'].apply(lambda x: ','.join(x))\n",
    "    news_clean = news_clean.drop_duplicates(subset=['V2Tone', 'Themes', 'Organizations'], keep='first')\n",
    "    news_clean['Organizations'] = news_clean['Organizations'].apply(lambda x: x.split(','))\n",
    "    # Return the first value in each entry from v2tone column\n",
    "    news_clean['V2Tone'] = news_clean['V2Tone'].apply(lambda x: return_first_v2tone_score(x))\n",
    "\n",
    "    return news_clean\n",
    "\n",
    "def process_theme(news_clean):\n",
    "    final_themes = []\n",
    "    for i in range(len(news_clean['Themes'])):\n",
    "        needed_themes = []\n",
    "        row_themes = news_clean['Themes'].str.split(';').iloc[i]\n",
    "        for j in row_themes:\n",
    "            if j in themes:\n",
    "                needed_themes.append(j)\n",
    "        final_themes.append(needed_themes)\n",
    "\n",
    "    news_clean['FinalThemes'] = final_themes\n",
    "    news_clean['FinalThemes'] = [','.join(map(str, l)) for l in news_clean['FinalThemes']]\n",
    "    news_clean['FinalThemes'] = news_clean['FinalThemes'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    news_clean = news_clean[news_clean['FinalThemes'].notna()]\n",
    "\n",
    "    return news_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0be50ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'company_dict_MXUS.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21936/1969888818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Selecting companies, themes, nations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'company_dict_MXUS.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcompany_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mkey_temp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\anaconda\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# 1) try standard library Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'company_dict_MXUS.pickle'"
     ]
    }
   ],
   "source": [
    "# Selecting companies, themes, nations\n",
    "df = pd.read_pickle('company_dict_MXUS.pickle')\n",
    "company_dict = {}\n",
    "for key,value in df.items():\n",
    "    key_temp=key.replace(',',' ')\n",
    "    company_dict[key_temp.lower()] = value.lower()\n",
    "company_key = list(company_dict.keys())\n",
    "company_key1=','.join(company_key)\n",
    "# print(\"company_key1\",company_key)\n",
    "df_theme = pd.read_pickle('theme_sdg_mapping.pk')\n",
    "themes = list(df_theme.keys())\n",
    "\n",
    "redundant = ['co', 'plc', 'ltd', '&', 'inc', 'company', 'corp', 'corporation']\n",
    "\n",
    "nations = set(['afghanistan', 'albania', 'algeria', 'america', 'andorra', 'angola', 'antigua', 'argentina', 'armenia',\n",
    "               'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus',\n",
    "               'belgium', 'belize', 'benin', 'bhutan', 'bissau', 'bolivia', 'bosnia', 'botswana', 'brazil', 'british',\n",
    "               'brunei', 'bulgaria', 'burkina', 'burma', 'burundi', 'cambodia', 'cameroon', 'canada', 'cape verde',\n",
    "               'central african republic', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa rica',\n",
    "               'country debt', 'croatia', 'cuba', 'cyprus', 'czech', 'denmark', 'djibouti', 'dominica', 'east timor',\n",
    "               'ecuador', 'egypt', 'el salvador', 'emirate', 'england', 'eritrea', 'estonia', 'ethiopia', 'fiji',\n",
    "               'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana', 'great britain', 'greece',\n",
    "               'grenada', 'grenadines', 'guatemala', 'guinea', 'guyana', 'haiti', 'herzegovina', 'honduras', 'hungary',\n",
    "               'iceland', 'in usa', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'ivory coast',\n",
    "               'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'kiribati', 'korea', 'kosovo', 'kuwait',\n",
    "               'kyrgyzstan', 'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'lithuania',\n",
    "               'luxembourg', 'macedonia', 'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'marshall',\n",
    "               'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'monaco', 'mongolia', 'montenegro',\n",
    "               'morocco', 'mozambique', 'myanmar', 'namibia', 'nauru', 'nepal', 'netherlands', 'new zealand',\n",
    "               'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'panama', 'papua', 'paraguay',\n",
    "               'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'saint kitts',\n",
    "               'samoa', 'san marino', 'santa lucia', 'sao tome', 'saudi arabia', 'scotland', 'scottish', 'senegal',\n",
    "               'serbia', 'seychelles', 'sierra leone', 'singapore', 'slovakia', 'slovenia', 'solomon', 'somalia',\n",
    "               'south africa', 'south sudan', 'spain', 'sri lanka', 'st kitts', 'st lucia', 'st. kitts', 'st. lucia',\n",
    "               'sudan', 'suriname', 'swaziland', 'sweden', 'switzerland', 'syria', 'taiwan', 'tajikistan', 'tanzania',\n",
    "               'thailand', 'tobago', 'togo', 'tonga', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'tuvalu',\n",
    "               'uganda', 'ukraine', 'united kingdom', 'united states', 'uruguay', 'usa', 'uzbekistan', 'vanuatu',\n",
    "               'vatican', 'venezuela', 'vietnam', 'wales', 'welsh', 'yemen', 'zambia', 'zimbabwe'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 2 log file names, change url time span, change dir if needed\n",
    "\n",
    "# Download the csv file\n",
    "logging.basicConfig(filename='download_log_2024_Jan-May.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('download_log_2024_Jan-May.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logging.getLogger('').addHandler(file_handler)\n",
    "processed_urls = set()  # store processed URLs\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def download_process(url):\n",
    "    try:\n",
    "        name = url.split(\"/\")[-1]\n",
    "        table_name = name.split(\".\")[0]\n",
    "\n",
    "        start_time = time.time()  \n",
    "        results, name, names = process_url(url)\n",
    "        end_time = time.time()  \n",
    "        download_time = end_time - start_time  \n",
    "        with lock:\n",
    "            logging.info(f\"Download time for {name}: {download_time} seconds\")\n",
    "\n",
    "        data = clean_raw_data(results)\n",
    "        final_all = process_theme(data)\n",
    "        final_all.to_csv(f'/Volumes/T5EVO/gdelt_download/2024/{names[0]}.csv', index=True, header=True)\n",
    "\n",
    "        with lock:\n",
    "            log_message = f\"Successfully downloaded from URL: {url} at {datetime.now()}\"\n",
    "            logging.info(log_message)\n",
    "    except Exception as e:\n",
    "        with lock:\n",
    "            error_message = f\"Error processing URL: {url} - {e}\"\n",
    "            logging.error(error_message)\n",
    "\n",
    "def main():\n",
    "    #urls = generate_gdelt_urls(2024,3,3, 2024,6,1)\n",
    "    urls = [\"http://data.gdeltproject.org/gdeltv2/20230921234500.gkg.csv.zip\",\"http://data.gdeltproject.org/gdeltv2/20231010050000.gkg.csv.zip\"]\n",
    "    print(\"Total processing urls: \", len(urls))\n",
    "\n",
    "    max_cores = psutil.cpu_count(logical=False)\n",
    "    print(f\"Using {max_cores} CPU cores for processing.\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_cores) as executor:\n",
    "        executor.map(download_process, urls)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing table names\n",
    "\n",
    "root_dir = '/Volumes/T5EVO/gdelt_download/2017'\n",
    "schema_name = 'gdelt'\n",
    "# Database connection URL\n",
    "database_url = \"postgresql://postgres@localhost:5432/gdelt\"\n",
    "try:\n",
    "    engine = create_engine(database_url)\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connection successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n",
    "\n",
    "# Get a list of all CSV files in the root directory\n",
    "csv_files = [file[:-4] for file in os.listdir(root_dir) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize a list to store missing tables\n",
    "missing_tables = []\n",
    "\n",
    "# Check if each table exists in the database\n",
    "for table_name in csv_files:\n",
    "    sql = f\"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'gdelt' AND table_name = '{table_name}.csv');\"\n",
    "    result = engine.execute(sql)\n",
    "    exists = result.scalar()\n",
    "    if not exists:\n",
    "        missing_tables.append(table_name + \".csv\")\n",
    "\n",
    "# Output missing tables\n",
    "if missing_tables:\n",
    "    print(\"\\nTables not found in the database:\")\n",
    "    print(len(missing_tables))\n",
    "else:\n",
    "    print(\"\\nAll tables are in the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert list with missing tables\n",
    "\n",
    "root_dir = '/Volumes/T5EVO/gdelt_download/2017/'\n",
    "database_url = \"postgresql://postgres@localhost:5432/gdelt\"\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    engine = create_engine(database_url)\n",
    "    print(\"Database connection successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Database connection failed:\", e)\n",
    "    exit()\n",
    "\n",
    "logging.basicConfig(filename='insertion_log_2017.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove any existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Add a new file handler\n",
    "file_handler = logging.FileHandler('insertion_log_2017.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "def copy_to_database(df, database_url, schema_name, table_name):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        table_name = f'\"{schema_name}\".\"{table_name}.csv\"'\n",
    "\n",
    "        # Define column names and their corresponding data types\n",
    "        column_types = {\n",
    "            'GKGRECORDID': 'TEXT',\n",
    "            'DATE': 'TEXT',\n",
    "            'SourceCollectionIdentifier': 'TEXT',\n",
    "            'SourceCommonName': 'TEXT',\n",
    "            'DocumentIdentifier': 'TEXT',\n",
    "            'Counts': 'TEXT',\n",
    "            'V2Counts': 'TEXT',\n",
    "            'Themes': 'TEXT',\n",
    "            'V2Themes': 'TEXT',\n",
    "            'Locations': 'TEXT',\n",
    "            'V2Locations': 'TEXT',\n",
    "            'Persons': 'TEXT',\n",
    "            'V2Persons': 'TEXT',\n",
    "            'Organizations': 'TEXT',\n",
    "            'V2Organizations': 'TEXT',\n",
    "            'V2Tone': 'FLOAT8', \n",
    "            'Dates': 'TEXT',\n",
    "            'GCAM': 'TEXT',\n",
    "            'SharingImage': 'TEXT',\n",
    "            'RelatedImages': 'TEXT',\n",
    "            'SocialImageEmbeds': 'TEXT',\n",
    "            'SocialVideoEmbeds': 'TEXT',\n",
    "            'Quotations': 'TEXT',\n",
    "            'AllNames': 'TEXT',\n",
    "            'Amounts': 'TEXT',\n",
    "            'TranslationInfo': 'TEXT',\n",
    "            'Extras': 'TEXT',\n",
    "            'FinalThemes': 'TEXT'\n",
    "        }\n",
    "\n",
    "        # Generate the CREATE TABLE query based on column names and types\n",
    "        create_table_sql = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n\"\n",
    "        for col, col_type in column_types.items():\n",
    "            create_table_sql += f\"{col} {col_type},\\n\"\n",
    "        create_table_sql = create_table_sql.rstrip(',\\n') + \"\\n)\"\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            with conn.begin():\n",
    "                conn.execute(create_table_sql)\n",
    "                \n",
    "\n",
    "                copy_sql = f\"\"\"\n",
    "                    COPY {table_name}\n",
    "                    FROM STDIN\n",
    "                    WITH CSV HEADER DELIMITER AS '\\t'\n",
    "                \"\"\"\n",
    "\n",
    "                # Write DataFrame to a buffer\n",
    "                buffer = StringIO()\n",
    "                df.to_csv(buffer, sep='\\t', header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "                buffer.seek(0)\n",
    "                print(f\"DataFrame written to buffer for {table_name}\")\n",
    "\n",
    "                # Copy data from buffer to database\n",
    "                conn.connection.cursor().copy_expert(copy_sql, buffer)\n",
    "                print(f\"Data copied to table {table_name}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        insertion_time = end_time - start_time\n",
    "        logging.info(f\"Insertion time for {table_name}: {insertion_time} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error copying data to database for table {table_name}: {e}\"\n",
    "        logging.error(error_message)\n",
    "        print(\"copy_to_database error: \", error_message)\n",
    "        raise  # Re-raise the exception to identify the specific line of the error\n",
    "\n",
    "def process_csv(file_path):\n",
    "    try:\n",
    "        print(f\"Starting process_csv for file: {file_path}\")\n",
    "        name = os.path.basename(file_path)\n",
    "        year = name.split(\".\")[0][:4]\n",
    "\n",
    "\n",
    "        table_name = name.split(\".\")[0]\n",
    "\n",
    "        inspector = inspect(engine)\n",
    "        schema_name = \"gdelt\"\n",
    "        if inspector.has_table(table_name, schema=schema_name):\n",
    "            logging.info(f\"Table {table_name} skipped because it exists.\")\n",
    "            return\n",
    "\n",
    "        # Read the CSV file using pandas within a context manager\n",
    "        df = pd.read_csv(file_path).drop([\"Unnamed: 0\"], axis=1)\n",
    "        print(f\"CSV file {file_path} read into DataFrame\")\n",
    "\n",
    "        # Process the CSV file and insert data into the database\n",
    "        copy_to_database(df, database_url, schema_name, table_name)\n",
    "\n",
    "        log_message = f\"Successfully wrote table from file: {file_path} at {datetime.now()}\"\n",
    "        logging.info(log_message)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing file: {file_path} - {e}\"\n",
    "        logging.error(error_message)\n",
    "        print(\"process_csv error: \", error_message)\n",
    "        raise  # Re-raise the exception to identify the specific line of the error\n",
    "        \n",
    "# Create full file paths by joining the root directory and file names\n",
    "full_file_paths = [f\"{root_dir}{file_name}\" for file_name in missing_tables]\n",
    "print(\"len of processing csv: \",len(full_file_paths))\n",
    "\n",
    "# Insert CSV files not found in the database\n",
    "if full_file_paths:\n",
    "    for file_path in full_file_paths:\n",
    "        try:\n",
    "            process_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {file_path} - {e}\")\n",
    "else:\n",
    "    print(\"All CSV files are already in the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142bc46",
   "metadata": {},
   "source": [
    "The following is to change all files name ending with \".csv\" as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change table name (add '.csv')\n",
    "\n",
    "database_url = \"postgresql://postgres@localhost:5432/gdelt\"\n",
    "try:\n",
    "    conn = psycopg2.connect(database_url)\n",
    "    print(\"Database connection successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Database connection failed:\", e)\n",
    "    exit()\n",
    "    \n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query to get table names that do not end with '.csv'\n",
    "query = \"\"\"\n",
    "SELECT table_name \n",
    "FROM information_schema.tables \n",
    "WHERE table_schema = 'gdelt' AND table_name NOT LIKE '%.csv';\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Execute the query\n",
    "    cur.execute(query)\n",
    "\n",
    "    # Fetch all the table names\n",
    "    table_names = cur.fetchall()\n",
    "\n",
    "    if table_names:\n",
    "        print(\"Tables that do not end with '.csv':\")\n",
    "        for table in table_names:\n",
    "            print(table[0])\n",
    "            \n",
    "        # Rename tables\n",
    "        for table in table_names:\n",
    "            old_table_name = table[0]\n",
    "            new_table_name = old_table_name + '.csv'\n",
    "            rename_query = sql.SQL(\"ALTER TABLE {}.{} RENAME TO {}\").format(\n",
    "                sql.Identifier('gdelt'),\n",
    "                sql.Identifier(old_table_name),\n",
    "                sql.Identifier(new_table_name)\n",
    "            )\n",
    "            cur.execute(rename_query)\n",
    "            print(f\"Renamed {old_table_name} to {new_table_name}\")\n",
    "            \n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "    else:\n",
    "        print(\"No tables found that do not end with '.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error executing query or renaming tables:\", e)\n",
    "    conn.rollback()\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
