{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import threading\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import time\n",
    "from sqlalchemy import create_engine, inspect, text, MetaData\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import psycopg2.errors\n",
    "import psycopg2\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc9d4f",
   "metadata": {},
   "source": [
    "# clean local csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd96559",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = {\n",
    "    'GKGRECORDID': 'gkg_2.1_English_GKGRecordID',\n",
    "    'DATE': 'gkg_2.1_English_V2.1Date',\n",
    "    'SourceCollectionIdentifier': 'gkg_2.1_English_V2SourceCollectionIdentifier',\n",
    "    'SourceCommonName': 'gkg_2.1_English_V2SourceCommonName',\n",
    "    'DocumentIdentifier': 'gkg_2.1_English_V2DocumentIdentifier',\n",
    "    'Counts': 'gkg_2.1_English_V1Counts',\n",
    "    'V2Counts': 'gkg_2.1_English_V2.1Counts',\n",
    "    'Themes': 'gkg_2.1_English_V1Themes',\n",
    "    'V2Themes': 'gkg_2.1_English_V2EnhancedThemes',\n",
    "    'Locations': 'gkg_2.1_English_V1Locations',\n",
    "    'V2Locations': 'gkg_2.1_English_V2EnhancedLocations',\n",
    "    'Persons': 'gkg_2.1_English_V1Persons',\n",
    "    'V2Persons': 'gkg_2.1_English_V2EnhancedPersons',\n",
    "    'Organizations': 'gkg_2.1_English_V1Organizations',\n",
    "    'V2Organizations': 'gkg_2.1_English_V2EnhancedOrganizations',\n",
    "    'V2Tone': 'gkg_2.1_English_V2.1Tone',\n",
    "    'Dates': 'gkg_2.1_English_V2.1EnhancedDates',\n",
    "    'GCAM': 'gkg_2.1_English_V2GCAM',\n",
    "    'SharingImage': 'gkg_2.1_English_V2.1SharingImage',\n",
    "    'RelatedImages': 'gkg_2.1_English_V2.1RelatedImages',\n",
    "    'SocialImageEmbeds': 'gkg_2.1_English_V2.1SocialImageEmbeds',\n",
    "    'SocialVideoEmbeds': 'gkg_2.1_English_V2.1SocialVideoEmbeds',\n",
    "    'Quotations': 'gkg_2.1_English_V2.1Quotations',\n",
    "    'AllNames': 'gkg_2.1_English_V2.1AllNames',\n",
    "    'Amounts': 'gkg_2.1_English_V2.1Amounts',\n",
    "    'TranslationInfo': 'gkg_2.1_English_V2.1TranslationInfo',\n",
    "    'Extras': 'gkg_2.1_English_V2ExtrasXML',\n",
    "    'FinalThemes': 'gkg_2.1_English_V2FinalThemes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 & V2: Define the columns and their respective subcolumns\n",
    "column_mapping = {\n",
    "    'gkg_2.1_English_V1Counts': ([\n",
    "        'gkg_2.1_English_V1.CountsCountType', 'gkg_2.1_English_V1.CountsCount', 'gkg_2.1_English_V1.CountsObjectType', \n",
    "        'gkg_2.1_English_V1.CountsLocationType', 'gkg_2.1_English_V1.CountsLocationFullName', \n",
    "        'gkg_2.1_English_V1.CountsLocationCountryCode', 'gkg_2.1_English_V1.CountsLocationADM1Code', \n",
    "        'gkg_2.1_English_V1.CountsLocationLatitude', \n",
    "        'gkg_2.1_English_V1.CountsLocationLongitude', 'gkg_2.1_English_V1.CountsLocationFeatureID'\n",
    "    ], ';', '#'),\n",
    "    'gkg_2.1_English_V1Locations': ([\n",
    "        'gkg_2.1_English_V1Locations.LocationType', 'gkg_2.1_English_V1Locations.LocationFullName', \n",
    "        'gkg_2.1_English_V1Locations.LocationCountryCode', \n",
    "        'gkg_2.1_English_V1Locations.LocationADM1Code', 'gkg_2.1_English_V1Locations.LocationLatitude', \n",
    "        'gkg_2.1_English_V1Locations.LocationLongitude', 'gkg_2.1_English_V1Locations.LocationFeatureID'\n",
    "    ], ';', '#'),\n",
    "    'gkg_2.1_English_V2EnhancedLocations': ([\n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationType', 'gkg_2.1_English_V2EnhancedLocations.LocationFullName', \n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationCountryCode', \n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationADM1Code', \n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationLatitude', \n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationLongitude', \n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationFeatureID',\n",
    "        'gkg_2.1_English_V2EnhancedLocations.LocationADM2Code'\n",
    "    ], ';', '#'),\n",
    "    'gkg_2.1_English_V2.1EnhancedDates': ([\n",
    "        'gkg_2.1_English_V2.1EnhancedDates.Resolution', 'gkg_2.1_English_V2.1EnhancedDates.Month', \n",
    "        'gkg_2.1_English_V2.1EnhancedDates.Day', 'gkg_2.1_English_V2.1EnhancedDates.Year', \n",
    "        'gkg_2.1_English_V2.1EnhancedDates.Offset'\n",
    "    ], ';', '#'),\n",
    "    'gkg_2.1_English_V2.1Quotations': ([\n",
    "        'gkg_2.1_English_V2.1Quotations.Offset', 'gkg_2.1_English_V2.1Quotations.Length',\n",
    "        'gkg_2.1_English_V2.1Quotations.Verb', 'gkg_2.1_English_V2.1Quotations.Quote'\n",
    "    ], '#', '|'),\n",
    "    'gkg_2.1_English_V2.1Amounts': ([\n",
    "        'gkg_2.1_English_V2.1Amounts.Amount', 'gkg_2.1_English_V2.1Amounts.Object', \n",
    "        'gkg_2.1_English_V2.1Amounts.Offset'\n",
    "    ], ';', ','),\n",
    "    'gkg_2.1_English_V2.1TranslationInfo': ([\n",
    "        'gkg_2.1_English_V2.1TranslationInfo.SRCLC', 'gkg_2.1_English_V2.1TranslationInfo.ENG'\n",
    "    ], ';', ':'),\n",
    "    'gkg_2.1_English_V2ExtrasXML': ([\n",
    "        'gkg_2.1_English_V2ExtrasXML.Authors', 'gkg_2.1_English_V2ExtrasXML.Title', \n",
    "        'gkg_2.1_English_V2ExtrasXML.BookTitle', 'gkg_2.1_English_V2ExtrasXML.Date', \n",
    "        'gkg_2.1_English_V2ExtrasXML.Journal', \n",
    "        'gkg_2.1_English_V2ExtrasXML.Volume', 'gkg_2.1_English_V2ExtrasXML.Issue', \n",
    "        'gkg_2.1_English_V2ExtrasXML.Pages', 'gkg_2.1_English_V2ExtrasXML.Institution', \n",
    "        'gkg_2.1_English_V2ExtrasXML.Publisher', \n",
    "        'gkg_2.1_English_V2ExtrasXML.Location', 'gkg_2.1_English_V2ExtrasXML.Marker'\n",
    "    ], ';', ','),\n",
    "    'gkg_2.1_English_V2.1Counts': ([\n",
    "        'gkg_2.1_English_V2.1.CountsCountType', 'gkg_2.1_English_V2.1.CountsCount', \n",
    "        'gkg_2.1_English_V2.1.CountsObjectType', \n",
    "        'gkg_2.1_English_V2.1.CountsLocationType', 'gkg_2.1_English_V2.1.CountsLocationFullName', \n",
    "        'gkg_2.1_English_V2.1.CountsLocationCountryCode', 'gkg_2.1_English_V2.1.CountsLocationADM1Code', \n",
    "        'gkg_2.1_English_V2.1.CountsLocationLatitude', \n",
    "        'gkg_2.1_English_V2.1.CountsLocationLongitude', 'gkg_2.1_English_V2.1.CountsLocationFeatureID'\n",
    "    ], ';', '#')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all designated columns\n",
    "def parse_columns_to_subcolumns(df, column_mapping):\n",
    "    \"\"\"\n",
    "    Parse specified columns into subcolumns based on provided mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - column_mapping: Dictionary where keys are the names of columns to parse,\n",
    "                      and values are tuples of (subcolumns, delimiter_block, delimiter_field).\n",
    "\n",
    "    Returns:\n",
    "    - df: Updated DataFrame with parsed subcolumns.\n",
    "    \"\"\"\n",
    "    for column_name, (subcolumns, delimiter_block, delimiter_field) in column_mapping.items():\n",
    "        # Add new subcolumns to the DataFrame\n",
    "        for subcolumn in subcolumns:\n",
    "            df[subcolumn] = ''\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for idx, row in df.iterrows():\n",
    "            example_data = row[column_name]\n",
    "\n",
    "            if pd.isna(example_data) or example_data == '':\n",
    "                continue\n",
    "\n",
    "            # Split the data into blocks\n",
    "            blocks = example_data.split(delimiter_block)\n",
    "\n",
    "            # Initialize lists to collect parsed subcolumn data\n",
    "            parsed_subcolumns = {subcolumn: [] for subcolumn in subcolumns}\n",
    "\n",
    "            # Process each block\n",
    "            for block in blocks:\n",
    "                if block:\n",
    "                    subfeature_values = block.split(delimiter_field)[1:]  # Skip the first empty split before the first '#'\n",
    "\n",
    "                    # Ensure we have the correct number of subfeatures\n",
    "                    subfeature_values = (subfeature_values + [''] * len(subcolumns))[:len(subcolumns)]\n",
    "\n",
    "                    # Append subfeature values to corresponding lists\n",
    "                    for subcolumn, value in zip(subcolumns, subfeature_values):\n",
    "                        parsed_subcolumns[subcolumn].append(value)\n",
    "\n",
    "            # Concatenate multiple values with ';' unless all values are empty\n",
    "            for subcolumn in subcolumns:\n",
    "                if all(value == '' for value in parsed_subcolumns[subcolumn]):\n",
    "                    df.at[idx, subcolumn] = ''\n",
    "                else:\n",
    "                    df.at[idx, subcolumn] = ';'.join(parsed_subcolumns[subcolumn])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 1 log name and 2 dir if needed\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='parse_log_2020.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "error_file_lock = threading.Lock()\n",
    "counter_lock = threading.Lock()\n",
    "processed_counter = 0 # Counter for processed files\n",
    "\n",
    "# Function to modify a CSV file and save it to a new file\n",
    "def modify_and_save_csv(input_file, output_file):\n",
    "    global processed_counter\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, index_col=0)#.drop(['Unnamed: 0'],axis=1)\n",
    "        \n",
    "        df.rename(columns=new_column_names, inplace=True) # change ori column name\n",
    "        df = parse_columns_to_subcolumns(df, column_mapping) # add subcolumns\n",
    "        df.fillna('None', inplace=True) # fill in blank cells\n",
    "        df.replace('', 'None', inplace=True)\n",
    "        df.to_csv(output_file, index=True, header=True)\n",
    "\n",
    "        logger.info(f\"Successfully modified CSV file: {input_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error modifying CSV file {input_file}: {str(e)}\")\n",
    "        \n",
    "        file_name = os.path.basename(input_file)\n",
    "        with error_file_lock:\n",
    "            with open('error_parse.txt', 'a') as f:\n",
    "                f.write(file_name + '\\n')\n",
    "\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        logger.info(f\"Modification operation took {duration:.2f} seconds for file: {input_file}\")\n",
    "        \n",
    "        with counter_lock:\n",
    "            processed_counter += 1\n",
    "            if processed_counter % 100 == 0:\n",
    "                print(f\"Processed {processed_counter} files.\")\n",
    "    \n",
    "input_directory = '/Volumes/T5EVO/gdelt_download/2020'\n",
    "output_directory = '/Volumes/T5EVO/gdelt_download/2020_clean'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through CSV files in the input directory\n",
    "csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n",
    "\n",
    "max_cores = psutil.cpu_count(logical=False)\n",
    "print(f\"Using {max_cores} CPU cores for processing.\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_cores) as executor:\n",
    "    for csv_file in csv_files:\n",
    "        file_name = os.path.basename(csv_file)\n",
    "        output_file = os.path.join(output_directory, file_name)\n",
    "        modify_and_save_csv(csv_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42028a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4c5e99a",
   "metadata": {},
   "source": [
    "# Insert Modified tables into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1491604",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    'gkg_2.1_English_GKGRecordID': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Date': 'BIGINT',\n",
    "    'gkg_2.1_English_V2SourceCollectionIdentifier': 'INT',\n",
    "    'gkg_2.1_English_V2SourceCommonName': 'TEXT',\n",
    "    'gkg_2.1_English_V2DocumentIdentifier': 'TEXT',\n",
    "    'gkg_2.1_English_V1Counts':'TEXT',\n",
    "    'gkg_2.1_English_V2.1Counts':'TEXT',\n",
    "    'gkg_2.1_English_V1Themes':'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedThemes':'TEXT',\n",
    "    'gkg_2.1_English_V1Locations':'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations':'TEXT',\n",
    "    'gkg_2.1_English_V1Persons':'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedPersons':'TEXT',\n",
    "    'gkg_2.1_English_V1Organizations':'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedOrganizations':'TEXT',\n",
    "    'gkg_2.1_English_V2.1Tone': 'FLOAT8',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates':'TEXT',\n",
    "    'gkg_2.1_English_V2GCAM':'TEXT',\n",
    "    'gkg_2.1_English_V2.1SharingImage':'TEXT',\n",
    "    'gkg_2.1_English_V2.1RelatedImages':'TEXT',\n",
    "    'gkg_2.1_English_V2.1SocialImageEmbeds':'TEXT',\n",
    "    'gkg_2.1_English_V2.1SocialVideoEmbeds':'TEXT',\n",
    "    'gkg_2.1_English_V2.1Quotations': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1AllNames':'TEXT',\n",
    "    'gkg_2.1_English_V2.1Amounts': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1TranslationInfo':'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML': 'TEXT',\n",
    "    'gkg_2.1_English_V2FinalThemes':'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsCountType': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsCount': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsObjectType': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationType': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationFullName': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationCountryCode': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationADM1Code': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationLatitude': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationLongitude': 'TEXT',\n",
    "    'gkg_2.1_English_V1.CountsLocationFeatureID': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationType': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationFullName': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationCountryCode': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationADM1Code': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationLatitude': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationLongitude': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationFeatureID': 'TEXT',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationADM2Code': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationType': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationFullName': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationCountryCode': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationADM1Code': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationLatitude': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationLongitude': 'TEXT',\n",
    "    'gkg_2.1_English_V1Locations.LocationFeatureID': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Resolution': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Month': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Day': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Year': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Offset': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Quotations.Offset': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Quotations.Length': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Quotations.Verb': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Quotations.Quote': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Amounts.Amount': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Amounts.Object': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1Amounts.Offset': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1TranslationInfo.SRCLC': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1TranslationInfo.ENG': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Authors': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Title': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.BookTitle': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Date': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Journal': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Volume': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Issue': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Pages': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Institution': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Publisher': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Location': 'TEXT',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Marker': 'TEXT',    \n",
    "    'gkg_2.1_English_V2.1.CountsCountType': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsCount': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsObjectType': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationType': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationFullName': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationCountryCode': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationADM1Code': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationLatitude': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationLongitude': 'TEXT',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationFeatureID': 'TEXT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_database(df, database_url, schema_name, table_name):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "\n",
    "        # Generate the CREATE TABLE query based on column names and types\n",
    "        create_table_sql = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n\"\n",
    "        for col, col_type in column_types.items():\n",
    "            create_table_sql += f'\"{col}\" {col_type},\\n'\n",
    "        create_table_sql = create_table_sql.rstrip(',\\n') + \"\\n)\"\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            with conn.begin():\n",
    "                '''\n",
    "                # Check if the schema exists\n",
    "                schema_exists = conn.execute(\n",
    "                    f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = '{schema_name}'\"\n",
    "                ).fetchone()\n",
    "                # Create schema if it does not exist\n",
    "                if not schema_exists:\n",
    "                    create_schema_sql = f'CREATE SCHEMA \"{schema_name}\"'\n",
    "                    conn.execute(create_schema_sql)\n",
    "                '''\n",
    "                    \n",
    "                # Create the table\n",
    "                conn.execute(create_table_sql)\n",
    "\n",
    "                copy_sql = f\"\"\"\n",
    "                    COPY {table_name}\n",
    "                    FROM STDIN\n",
    "                    WITH CSV HEADER DELIMITER AS '\\t'\n",
    "                \"\"\"\n",
    "\n",
    "                # Write DataFrame to a buffer\n",
    "                buffer = StringIO()\n",
    "                df.to_csv(buffer, sep='\\t', header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "                buffer.seek(0)\n",
    "                #print(f\"DataFrame written to buffer for {table_name}\")\n",
    "\n",
    "                # Copy data from buffer to database\n",
    "                conn.connection.cursor().copy_expert(copy_sql, buffer)\n",
    "                #print(f\"Data copied to table {table_name}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        insertion_time = end_time - start_time\n",
    "        logging.info(f\"Insertion time for {table_name}: {insertion_time} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error copying data to database for table {table_name}: {e}\"\n",
    "        logging.error(error_message)\n",
    "        print(\"copy_to_database error: \", error_message)\n",
    "        with open('error_parse_insertion.txt', 'a') as error_file:\n",
    "            error_file.write(f\"{file_path} copy_to_database error: {error_message}\\n\")\n",
    "        raise\n",
    "        raise  \n",
    "\n",
    "def process_csv(file_path, schema_name):\n",
    "    global processed_files\n",
    "    try:\n",
    "        #print(f\"Starting process_csv for file: {file_path}\")\n",
    "        name = os.path.basename(file_path)\n",
    "        #year = name.split(\".\")[0][:4]\n",
    "\n",
    "        table_name = name.split(\".\")[0]\n",
    "\n",
    "        inspector = inspect(engine)\n",
    "        if inspector.has_table(table_name, schema=schema_name):\n",
    "            logging.info(f\"Table {table_name} skipped because it exists.\")\n",
    "            return\n",
    "\n",
    "        # Read the CSV file using pandas within a context manager\n",
    "        df = pd.read_csv(file_path).drop([\"Unnamed: 0\"], axis=1)\n",
    "        #print(f\"CSV file {file_path} read into DataFrame\")\n",
    "        \n",
    "        # Check if DataFrame columns match the expected column types\n",
    "        if set(df.columns) != set(column_types.keys()):\n",
    "            error_message = f\"Skipping file {file_path}: columns do not match expected schema.\"\n",
    "            logging.error(error_message)\n",
    "            print(error_message)\n",
    "            with open('error_parse_insertion.txt', 'a') as error_file:\n",
    "                error_file.write(f\"{file_path}\\n\")\n",
    "            return\n",
    "\n",
    "        copy_to_database(df, database_url, schema_name, table_name)\n",
    "\n",
    "        log_message = f\"Successfully wrote table from file: {file_path} at {datetime.now()}\"\n",
    "        logging.info(log_message)\n",
    "        \n",
    "        processed_files += 1\n",
    "        if processed_files % 100 == 0:\n",
    "            print(f\"Processed {processed_files} files so far.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing file: {file_path} - {e}\"\n",
    "        logging.error(error_message)\n",
    "        print(\"process_csv error: \", error_message)\n",
    "        with open('error_parse_insertion.txt', 'a') as error_file:\n",
    "            error_file.write(f\"{file_path} process_csv error: {error_message}\\n\")\n",
    "        raise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac17455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'root_dir', 2 log names, and 1 'schema_name'\n",
    "\n",
    "def job(root_dir, schema_name):\n",
    "    global processed_files\n",
    "    all_csv_files = []\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                all_csv_files.append(os.path.join(subdir, file))\n",
    "    \n",
    "    max_cores = psutil.cpu_count(logical=False)\n",
    "    print(f\"Using {max_cores} CPU cores for processing.\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_cores) as executor:\n",
    "        executor.map(lambda file_path: process_csv(file_path, schema_name), all_csv_files)\n",
    "    #with ThreadPoolExecutor(max_workers=max_cores) as executor:\n",
    "    #    for file_path in all_csv_files:\n",
    "    #        executor.submit(process_csv, file_path, schema_name)\n",
    "    save_processed_files_count(processed_files)\n",
    "\n",
    "def save_processed_files_count(count):\n",
    "    with open('processed_files.txt', 'w') as file:\n",
    "        file.write(f\"Processed files: {count}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "    try:\n",
    "        engine = create_engine(database_url)\n",
    "        with engine.connect() as connection:\n",
    "            print(\"Connection successful.\")\n",
    "    except Exception as e:\n",
    "        print(\"Connection failed:\", e)\n",
    "    \n",
    "    processed_files = 0\n",
    "    \n",
    "    logging.basicConfig(filename='parse_insertion_log_2017.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    file_handler = logging.FileHandler('parse_insertion_log_2017.txt')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    #root_dir = '/Volumes/T5EVO/gdelt_download/2017_clean'\n",
    "    root_dir = '/Volumes/T5EVO/gdelt_download/missing_zip_empty_csv/empty_2017'\n",
    "    schema_name = \"gdelt_english\"\n",
    "    job(root_dir, schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb324773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d141ea63",
   "metadata": {},
   "source": [
    "# create empty csv from missing timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'gkg_2.1_English_GKGRecordID',\n",
    "    'gkg_2.1_English_V2.1Date',\n",
    "    'gkg_2.1_English_V2SourceCollectionIdentifier',\n",
    "    'gkg_2.1_English_V2SourceCommonName',\n",
    "    'gkg_2.1_English_V2DocumentIdentifier',\n",
    "    'gkg_2.1_English_V1Counts',\n",
    "    'gkg_2.1_English_V2.1Counts',\n",
    "    'gkg_2.1_English_V1Themes',\n",
    "    'gkg_2.1_English_V2EnhancedThemes',\n",
    "    'gkg_2.1_English_V1Locations',\n",
    "    'gkg_2.1_English_V2EnhancedLocations',\n",
    "    'gkg_2.1_English_V1Persons',\n",
    "    'gkg_2.1_English_V2EnhancedPersons',\n",
    "    'gkg_2.1_English_V1Organizations',\n",
    "    'gkg_2.1_English_V2EnhancedOrganizations',\n",
    "    'gkg_2.1_English_V2.1Tone',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates',\n",
    "    'gkg_2.1_English_V2GCAM',\n",
    "    'gkg_2.1_English_V2.1SharingImage',\n",
    "    'gkg_2.1_English_V2.1RelatedImages',\n",
    "    'gkg_2.1_English_V2.1SocialImageEmbeds',\n",
    "    'gkg_2.1_English_V2.1SocialVideoEmbeds',\n",
    "    'gkg_2.1_English_V2.1Quotations',\n",
    "    'gkg_2.1_English_V2.1AllNames',\n",
    "    'gkg_2.1_English_V2.1Amounts',\n",
    "    'gkg_2.1_English_V2.1TranslationInfo',\n",
    "    'gkg_2.1_English_V2ExtrasXML',\n",
    "    'gkg_2.1_English_V2FinalThemes',\n",
    "    'gkg_2.1_English_V1.CountsCountType',\n",
    "    'gkg_2.1_English_V1.CountsCount',\n",
    "    'gkg_2.1_English_V1.CountsObjectType',\n",
    "    'gkg_2.1_English_V1.CountsLocationType',\n",
    "    'gkg_2.1_English_V1.CountsLocationFullName',\n",
    "    'gkg_2.1_English_V1.CountsLocationCountryCode',\n",
    "    'gkg_2.1_English_V1.CountsLocationADM1Code',\n",
    "    'gkg_2.1_English_V1.CountsLocationLatitude',\n",
    "    'gkg_2.1_English_V1.CountsLocationLongitude',\n",
    "    'gkg_2.1_English_V1.CountsLocationFeatureID',\n",
    "    'gkg_2.1_English_V2.1.CountsCountType',\n",
    "    'gkg_2.1_English_V2.1.CountsCount',\n",
    "    'gkg_2.1_English_V2.1.CountsObjectType',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationType',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationFullName',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationCountryCode',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationADM1Code',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationLatitude',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationLongitude',\n",
    "    'gkg_2.1_English_V2.1.CountsLocationFeatureID',\n",
    "    'gkg_2.1_English_V1Locations.LocationType',\n",
    "    'gkg_2.1_English_V1Locations.LocationFullName',\n",
    "    'gkg_2.1_English_V1Locations.LocationCountryCode',\n",
    "    'gkg_2.1_English_V1Locations.LocationADM1Code',\n",
    "    'gkg_2.1_English_V1Locations.LocationLatitude',\n",
    "    'gkg_2.1_English_V1Locations.LocationLongitude',\n",
    "    'gkg_2.1_English_V1Locations.LocationFeatureID',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationType',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationFullName',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationCountryCode',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationADM1Code',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationLatitude',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationLongitude',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationFeatureID',\n",
    "    'gkg_2.1_English_V2EnhancedLocations.LocationADM2Code',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Resolution',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Month',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Day',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Year',\n",
    "    'gkg_2.1_English_V2.1EnhancedDates.Offset',\n",
    "    'gkg_2.1_English_V2.1Quotations.Offset',\n",
    "    'gkg_2.1_English_V2.1Quotations.Length',\n",
    "    'gkg_2.1_English_V2.1Quotations.Verb',\n",
    "    'gkg_2.1_English_V2.1Quotations.Quote',\n",
    "    'gkg_2.1_English_V2.1Amounts.Amount',\n",
    "    'gkg_2.1_English_V2.1Amounts.Object',\n",
    "    'gkg_2.1_English_V2.1Amounts.Offset',\n",
    "    'gkg_2.1_English_V2.1TranslationInfo.SRCLC',\n",
    "    'gkg_2.1_English_V2.1TranslationInfo.ENG',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Authors',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Title',\n",
    "    'gkg_2.1_English_V2ExtrasXML.BookTitle',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Date',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Journal',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Volume',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Issue',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Pages',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Institution',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Publisher',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Location',\n",
    "    'gkg_2.1_English_V2ExtrasXML.Marker'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d799394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 1 'log_file_path' and 1 'csv_file_path'\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "log_file_path = 'create_missing_csv_log.txt'\n",
    "txt_file_path = '/Users/macglobalai/Desktop/Gdelt Database/log_record/missing_timestamp_log/error_urls_2017.txt'\n",
    "with open(txt_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# extract timestamp from txt lines\n",
    "timestamp_pattern = re.compile(r'(\\d{14})')\n",
    "\n",
    "# process each line and save the df as a CSV file with the extracted timestamp\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    for line in lines:\n",
    "        start_time = time.time()\n",
    "        match = timestamp_pattern.search(line)\n",
    "        if match:\n",
    "            timestamp = match.group(1)\n",
    "            csv_file_path = f'/Volumes/T5EVO/gdelt_download/missing_zip_empty_csv/empty_2017/{timestamp}.csv'\n",
    "            df.to_csv(csv_file_path, index=True)\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            log_file.write(f\"{timestamp} takes {elapsed_time:.4f} seconds\\n\")\n",
    "        else:\n",
    "            log_file.write(f\"Missing {line.strip()}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c5789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07804e9f",
   "metadata": {},
   "source": [
    "# check the timestamps numbers among sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54358b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gdelt_urls(start_year, start_month, start_day, end_year, end_month, end_day):\n",
    "    start_date = datetime(start_year, start_month, start_day)\n",
    "    current_date = datetime(end_year, end_month, end_day) \n",
    "\n",
    "    delta = current_date - start_date\n",
    "    total_intervals = int(delta.total_seconds() / 900)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for i in range(total_intervals):\n",
    "        interval_date = start_date + timedelta(minutes = 15 * i)\n",
    "\n",
    "        formatted_date = interval_date.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "        #url = f'http://data.gdeltproject.org/gdeltv2/{formatted_date}.gkg.csv.zip'\n",
    "        urls.append(formatted_date)\n",
    "\n",
    "    return urls\n",
    "\n",
    "urls=[]\n",
    "urls=generate_gdelt_urls(2017,1,1, 2018,1,1) #starts at 20150218230000\n",
    "len(urls),urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get local numbers and url numbers\n",
    "dir_clean = '/Volumes/T5EVO/gdelt_download/2017_clean'\n",
    "dir_missing_zip_empty_csv = '/Volumes/T5EVO/gdelt_download/missing_zip_empty_csv/empty_2017'\n",
    "\n",
    "def count_files(directory):\n",
    "    return len([file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))])\n",
    "\n",
    "def compare_counts(dir1, dir2, urls):\n",
    "    count_dir1 = count_files(dir1)\n",
    "    count_dir2 = count_files(dir2)\n",
    "    urls_count = len(urls)\n",
    "    \n",
    "    print(f\"Number of files in '{dir1}': {count_dir1}\")\n",
    "    print(f\"Number of files in '{dir2}': {count_dir2}\")\n",
    "    print(f\"Number of URLs: {urls_count}\")\n",
    "    \n",
    "    if (count_dir1+count_dir2) != urls_count:\n",
    "        print(f\"The number of files in total {count_dir1+count_dir2} does not match the number of URLs {urls_count}.\")\n",
    "    else:\n",
    "        print(f\"The number of files in total matches the number of URLs.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_counts(dir_clean, dir_missing_zip_empty_csv, urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644dea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the missing timestamp between local and generated urls\n",
    "\n",
    "dir_clean = '/Volumes/T5EVO/gdelt_download/2017'\n",
    "dir_missing_zip_empty_csv = '/Volumes/T5EVO/gdelt_download/missing_zip_empty_csv/empty_2017'\n",
    "\n",
    "# Function to extract file timestamps from a directory\n",
    "def get_file_timestamps(directory):\n",
    "    files = [file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n",
    "    timestamps = {os.path.splitext(file)[0] for file in files}\n",
    "    return timestamps\n",
    "\n",
    "# Function to extract timestamps from URLs\n",
    "def get_url_timestamps(urls):\n",
    "    timestamps = {os.path.splitext(os.path.basename(urlparse(url).path))[0] for url in urls}\n",
    "    return timestamps\n",
    "\n",
    "# Function to find missing timestamps\n",
    "def find_missing_timestamps(dir1, dir2, urls):\n",
    "    timestamps_dir1 = get_file_timestamps(dir1)\n",
    "    timestamps_dir2 = get_file_timestamps(dir2)\n",
    "    timestamps_urls = get_url_timestamps(urls)\n",
    "    \n",
    "    all_local_timestamps = timestamps_dir1.union(timestamps_dir2)\n",
    "    missing_timestamps = timestamps_urls - all_local_timestamps\n",
    "    \n",
    "    print(f\"{len(missing_timestamps)} Missing timestamps: {sorted(missing_timestamps)}\")\n",
    "    return missing_timestamps\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    missing_timestamps = find_missing_timestamps(dir_clean, dir_missing_zip_empty_csv, urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd77dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the missing timestamps between db and generated urls\n",
    "import os\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Database connection URL\n",
    "database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "\n",
    "# Function to extract timestamps from URLs\n",
    "def get_url_timestamps(urls):\n",
    "    timestamps = {os.path.splitext(os.path.basename(urlparse(url).path))[0] for url in urls}\n",
    "    return timestamps\n",
    "\n",
    "# Function to retrieve table names from the database schema\n",
    "def get_db_table_names(engine, schema_name):\n",
    "    inspector = inspect(engine)\n",
    "    table_names = inspector.get_table_names(schema=schema_name)\n",
    "    return set(table_names)\n",
    "\n",
    "# Function to find missing timestamps\n",
    "def find_missing_timestamps(db_table_names, urls):\n",
    "    timestamps_urls = get_url_timestamps(urls)\n",
    "    missing_timestamps = timestamps_urls - db_table_names\n",
    "    \n",
    "    print(f\"{len(missing_timestamps)} Missing timestamps: {sorted(missing_timestamps)}\")\n",
    "    return missing_timestamps\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a database engine\n",
    "    engine = create_engine(database_url)\n",
    "    \n",
    "    # Define the schema name\n",
    "    schema_name = \"2016\"\n",
    "    \n",
    "    # Retrieve table names from the database\n",
    "    db_table_names = get_db_table_names(engine, schema_name)\n",
    "    \n",
    "    # Find missing timestamps\n",
    "    missing_timestamps = find_missing_timestamps(db_table_names, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb92692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find out the missing timestamp between local and db\n",
    "\n",
    "database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "schema_name = \"2015\"\n",
    "csv_directory = '/Volumes/T5EVO/gdelt_download/aa'\n",
    "\n",
    "engine = create_engine(database_url)\n",
    "\n",
    "def get_tables_in_schema(engine, schema):\n",
    "    with engine.connect() as connection:\n",
    "        inspector = inspect(engine)\n",
    "        # Get the list of tables in the schema\n",
    "        tables = inspector.get_table_names(schema=schema)\n",
    "        return tables\n",
    "\n",
    "def get_csv_files(directory):\n",
    "    csv_files = [os.path.splitext(file)[0] for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    return csv_files\n",
    "\n",
    "def find_missing_tables(tables, csv_files):\n",
    "    # Find CSV files that are not in the list of tables\n",
    "    missing_tables = set(csv_files) - set(tables)\n",
    "    return list(missing_tables)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get list of tables in the schema\n",
    "    tables = get_tables_in_schema(engine, schema_name)\n",
    "    \n",
    "    # Get list of CSV files in the directory\n",
    "    csv_files = get_csv_files(csv_directory)\n",
    "    \n",
    "    # Find missing tables\n",
    "    missing_tables = find_missing_tables(tables, csv_files)\n",
    "    \n",
    "    print(f\"{len(missing_tables)} Missing tables (present as CSV files but not in schema '{schema_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf97ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9afa06e",
   "metadata": {},
   "source": [
    "# transfer tables between schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba925ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "source_schema = \"2017\"\n",
    "target_schema = \"gdelt_english\"\n",
    "\n",
    "engine = create_engine(database_url)\n",
    "\n",
    "logging.basicConfig(filename='move_log_2017.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def move_tables(engine, source_schema, target_schema):\n",
    "    with engine.connect() as connection:\n",
    "        inspector = inspect(engine)\n",
    "        \n",
    "        # Get the list of tables in the source schema\n",
    "        tables = inspector.get_table_names(schema=source_schema)\n",
    "        \n",
    "        for i, table in enumerate(tables, start=1):\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Generate the SQL command to move the table\n",
    "                move_table_sql = text(f\"\"\"\n",
    "                ALTER TABLE \"{source_schema}\".\"{table}\" SET SCHEMA \"{target_schema}\";\n",
    "                \"\"\")\n",
    "                \n",
    "                # Execute the SQL command\n",
    "                connection.execute(move_table_sql)\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                log_message = f\"Moved table {table} from schema {source_schema} to {target_schema} in {duration:.2f} seconds.\"\n",
    "                logger.info(log_message)\n",
    "                \n",
    "                if i % 1000 == 0:\n",
    "                    print(f\"Moved {i} tables so far.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_message = f\"Error moving table {table}: {e}\"\n",
    "                logger.error(error_message)\n",
    "                print(error_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    move_tables(engine, source_schema, target_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bd74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82bdc401",
   "metadata": {},
   "source": [
    "# Extra SQL operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fdbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change schema name\n",
    "\n",
    "database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "engine = create_engine(database_url)\n",
    "conn = engine.connect()\n",
    "sql_command = 'ALTER SCHEMA \"2017\" RENAME TO \"2018\";'\n",
    "conn.execute(sql_command)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date column from one example table\n",
    "\n",
    "def select_column(database_url, schema_name, table_name, column_name):\n",
    "    try:\n",
    "        # Create a database engine\n",
    "        engine = create_engine(database_url)\n",
    "        \n",
    "        # Define the full table name with schema\n",
    "        full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "        \n",
    "        # Define the SQL query to select the specific column\n",
    "        query = text(f'SELECT \"{column_name}\" FROM {full_table_name}')\n",
    "        \n",
    "        # Execute the query and fetch the results\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(query)\n",
    "            data = result.fetchall()\n",
    "            \n",
    "            # Print the retrieved data\n",
    "            for row in data:\n",
    "                print(row)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to select column {column_name} from table {schema_name}.{table_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "    schema_name = \"2015\"\n",
    "    table_name = \"20150218230000\"\n",
    "    column_name = \"gkg_2.1_English_V2.1Date\"\n",
    "\n",
    "    select_column(database_url, schema_name, table_name, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb358ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete specific table\n",
    "def delete_table(database_url, schema_name, table_name):\n",
    "    try:\n",
    "        engine = create_engine(database_url)\n",
    "        with engine.connect() as connection:\n",
    "            # Define the table name with schema\n",
    "            full_table_name = f'\"{schema_name}\".\"{table_name}\"'\n",
    "            \n",
    "            # SQL command to drop the table\n",
    "            drop_table_sql = f\"DROP TABLE IF EXISTS {full_table_name}\"\n",
    "            \n",
    "            # Execute the SQL command\n",
    "            connection.execute(drop_table_sql)\n",
    "            print(f\"Table {full_table_name} has been successfully deleted.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete table {schema_name}.{table_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    database_url = \"postgresql://postgres@localhost:5432/gdelt_english\"\n",
    "    schema_name = \"\"\n",
    "    table_name = \"20150218234500\"  \n",
    "    \n",
    "    delete_table(database_url, schema_name, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4367e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
